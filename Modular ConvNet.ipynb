{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signing-in\n",
    "\n",
    "Note that you will need to sign-in to use this notebook. Simply click on the `[â¡` icon on the extreme right of the title bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "from gzip import GzipFile\n",
    "from StringIO import StringIO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing MNIST format\n",
    "\n",
    "The files are gzipped and in the format described at http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labels(label_data_raw):\n",
    "  file = GzipFile(mode = 'r', fileobj = StringIO(label_data_raw))\n",
    "  (magic_number, num_items) = struct.unpack('>II', file.read(8))\n",
    "  assert magic_number == 2049, 'Not a label file!'\n",
    "  return struct.unpack('>' + 'B' * num_items, file.read(num_items))\n",
    "\n",
    "def read_images(image_data_raw):\n",
    "  file = GzipFile(mode = 'r', fileobj = StringIO(image_data_raw))\n",
    "  (magic_number, num_items, rows, cols) = struct.unpack('>IIII', file.read(16))\n",
    "  assert magic_number == 2051, 'Not an image file!'\n",
    "  assert rows == 28 and cols == 28, 'Not 28 x 28 images.'\n",
    "  num_pixels = rows * cols\n",
    "  images = []\n",
    "  for i in range(num_items):\n",
    "    images.append(map(lambda x: x / 255.0,\n",
    "                      struct.unpack('>' + 'B' * num_pixels, file.read(num_pixels))))\n",
    "  return images\n",
    "\n",
    "def to_one_hot(value):\n",
    "  result = [0.0] * 10\n",
    "  result[value] = 1.0\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "  image = [image[28*y : 28*(y+1)] for y in range(28)]\n",
    "  fig = plt.figure()\n",
    "  fig.set_figwidth(2)\n",
    "  fig.set_figheight(2)\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(image, cmap = matplotlib.cm.binary)\n",
    "  plt.xticks(np.array([]))\n",
    "  plt.yticks(np.array([]))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and parse MNIST data\n",
    "\n",
    "The data was uploaded in gzip format to a public GCP cloud storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%storage read --object gs://elementai-research-public/t10k-images-idx3-ubyte.gz -v test_images_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%storage read --object gs://elementai-research-public/t10k-labels-idx1-ubyte.gz -v test_labels_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%storage read --object gs://elementai-research-public/train-images-idx3-ubyte.gz -v train_images_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%storage read --object gs://elementai-research-public/train-labels-idx1-ubyte.gz -v train_labels_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = zip(read_images(test_images_raw),\n",
    "                map(to_one_hot, read_labels(test_labels_raw)))\n",
    "train_data = zip(read_images(train_images_raw),\n",
    "                 map(to_one_hot, read_labels(train_labels_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data validation\n",
    "\n",
    "Make sure we read everything correctly with a simple visual validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_image(train_data[59999][0])\n",
    "print(train_data[59999][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering\n",
    "\n",
    "Given `data = [(image, one_hot), ...]` create a `filtered_data = [(image, filtered_one_hot), ...]` of the same size. For each entry, `filtered_one_hot` only identifies the desired `digits` in the specified order. If less than the 10 digits are specified then an extra binary value is added at the end `filtered_one_hot` to capture the leftover digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_data(data, digits):\n",
    "  nb_digits = len(data[0][1])\n",
    "  assert all(x >= 0 and x < nb_digits for x in digits) , 'Digits must be between 0 and 10.'\n",
    "  assert all(x != y for x, y in zip(sorted(digits)[:-1], sorted(digits)[1:])), 'Digits must not repeat.'\n",
    "  res = [(image, [one_hot[i] for i in digits]) for image, one_hot in data]\n",
    "  if len(digits) == nb_digits:\n",
    "    return res\n",
    "  else:\n",
    "    return [(image, one_hot + [1.0 - sum(one_hot)]) for image, one_hot in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility TensorFlow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this to initialize or reset the session.\n",
    "try:\n",
    "  sess.close()\n",
    "  tf.reset_default_graph()\n",
    "except NameError:\n",
    "  pass\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "  def __init__(self, trainable_variables, x_placeholder, y_placeholder, \n",
    "               permutation_placeholder, minibatch_size, train_step_node, \n",
    "               accuracy_node):\n",
    "    global sess\n",
    "    # Initialize all variables except trainable variables that are not\n",
    "    # in trainable_variables.\n",
    "    variables_to_init = (set(tf.all_variables()) - set(tf.trainable_variables())) \\\n",
    "                         | set(trainable_variables)\n",
    "    sess.run(tf.initialize_variables(variables_to_init))\n",
    "    self.base_index = 0\n",
    "    self.accuracy = []\n",
    "    self.x_placeholder = x_placeholder\n",
    "    self.y_placeholder = y_placeholder\n",
    "    self.permutation_placeholder = permutation_placeholder\n",
    "    self.minibatch_size = minibatch_size\n",
    "    self.train_step_node = train_step_node\n",
    "    self.accuracy_node = accuracy_node\n",
    "\n",
    "  def __gen_feed_dict(self, xs, ys, permutation, extra_feed_params):\n",
    "    feed_dict = { \n",
    "      self.x_placeholder: xs, \n",
    "      self.y_placeholder: ys,\n",
    "      self.permutation_placeholder: permutation\n",
    "    }\n",
    "    for key, value in extra_feed_params.items():\n",
    "      feed_dict[key] = value\n",
    "    return feed_dict\n",
    "    \n",
    "  def train(self, train_data, num_steps, accuracy_every_n_steps = 50, \n",
    "            extra_feed_params = {}):\n",
    "    global sess\n",
    "    assert len(train_data) % self.minibatch_size == 0, 'Minibatch size must be a divider of len(train_data)'\n",
    "    for i in range(num_steps):\n",
    "      index = (i + self.base_index)\n",
    "      start_index = (index * self.minibatch_size) % len(train_data)\n",
    "      end_index = start_index + self.minibatch_size\n",
    "      batch_xs, batch_ys = zip(*train_data[start_index:end_index])\n",
    "      permutation = range(len(batch_ys[0]))\n",
    "      random.shuffle(permutation)\n",
    "      feed_dict = self.__gen_feed_dict(batch_xs, batch_ys, permutation, \n",
    "                                       extra_feed_params)\n",
    "      sess.run(self.train_step_node, feed_dict = feed_dict)\n",
    "      if index % accuracy_every_n_steps == 0:\n",
    "        curr_accuracy = float(sess.run(self.accuracy_node, feed_dict = feed_dict))\n",
    "        print 'Iteration = %d, Training accuracy = %f' % (index, curr_accuracy)\n",
    "        self.accuracy.append({ 'batch': index, 'accuracy': curr_accuracy })\n",
    "    self.base_index += num_steps\n",
    "    return self.accuracy\n",
    "\n",
    "  def test_accuracy(self, test_data, extra_feed_params = {}):\n",
    "    global sess\n",
    "    result = 0.0\n",
    "    num_batches = 100\n",
    "    batch_len = len(test_data) / num_batches\n",
    "    for i in range(num_batches):\n",
    "      start_index = batch_len * i\n",
    "      end_index = start_index + batch_len\n",
    "      test_xs, test_ys = zip(*test_data[start_index:end_index])\n",
    "      permutation = range(len(test_ys[0]))\n",
    "      feed_dict = self.__gen_feed_dict(test_xs, test_ys, permutation,\n",
    "                                       extra_feed_params)\n",
    "      result += sess.run(self.accuracy_node, feed_dict = feed_dict)\n",
    "    result = result / num_batches\n",
    "    print 'Test accuracy = %f' % result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_var(shape, name):\n",
    "  return tf.get_variable(name, initializer = tf.truncated_normal(shape, stddev = 0.1))\n",
    "\n",
    "def bias_var(shape, name):\n",
    "  return tf.get_variable(name, initializer = tf.constant(0.1, shape = shape))\n",
    "\n",
    "def conv_layer(input, filter, bias):\n",
    "  return tf.nn.relu(tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME') + bias)\n",
    "\n",
    "def pool_layer(input):\n",
    "  return tf.nn.max_pool(input, [1,2,2,1], [1,2,2,1], 'SAME')\n",
    "\n",
    "def fully_connected_layer(input, weight, bias):\n",
    "  return tf.nn.relu(tf.matmul(input, weight) + bias)\n",
    "\n",
    "def fully_connected_sigmoid_layer(input, weight, bias):\n",
    "  return tf.nn.sigmoid(tf.matmul(input, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular ConvNet\n",
    "\n",
    "The idea is to build a modular MNIST classifier. Each module is an expert responsible for identifying a subset of the MNIST classes. For example an expert could be a binary classifier able to recognize a given digit.\n",
    "\n",
    "In addition to the binary classification output, a module listens and talks onto a communication channel connecting all the modules. The goal is to allow modules to figure out an emerging _communication protocol_ that helps them inform one another and collectively converge on a better multi-class classification than each module taken independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why?\n",
    "\n",
    "The hope is that such an architecture will allow us to train experts somewhat independently allowing:\n",
    "- Each module to be significantly simpler than a full multi-class network ;\n",
    "- Each module, once the protocol is determined, to be trainable in parallel ;\n",
    "- The easy addition of new classes in the future at the cost of training new modules ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How?\n",
    "\n",
    "Training occurs in two stages: the **protocol definition** stage and the **expert addition** stage.\n",
    "\n",
    "During _protocol definition_ a collection of experts is trained on a small portion of the classes. As they are being trained a communication protocol emerges. After training, this protocol is frozen. What this means in practice is defined below.\n",
    "\n",
    "During _expert addition_, new experts are trained on the remaining classes, but these experts have to abide by the existing protocol. This is done by including a fraction of the already-trained experts while training the new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication mechanism\n",
    "\n",
    "Communication is a multi hop process. At each hop, a module reads _n_ signals from the channel (except on the first hop) and emits _m_ signals onto it (except on the last hop). The emitted signals are multiplexed onto the channel by an independent _multiplexing network_ which takes _k * m_ signals and generates the _n_ signals of the channel. The same multiplexing network is used at every hop. \n",
    "\n",
    "This multiplexing network is built in a modular fashion (see below) so that: 1) it can be trained during the _protocol definition_ while 2) allowing _k_ to be modified during the _expert addition_ phase or at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Cascading multiplexing\n",
    "\n",
    "The _multiplexing network_ is made of _k_ copies of an identical _multiplexing unit (MU)_ which is a fully connected layer taking _m + n_ input signals and emitting _n_ signals. First the _k_ expert modules are ordered. The first _MU_ takes the _m_ signals emitted by the first module together with _n_ zero signals and emits _n_ signals that are fed into the second _MU_ which multiplexes them with the _n_ signals emitted by the second module.\n",
    "\n",
    "The the order of the expert modules is randomized for each minibatch and for each hop to ensure we learn a general multiplexing unit.\n",
    "\n",
    "**TODO:** Should there be a different _multiplexing unit_ for the first hop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Hierarchical multiplexing\n",
    "\n",
    "The _multiplexing network_ is made of _k/2_ copies of a _first level multiplexing unit (FLMU)_ and _lg(k/2)_ copies of an _internal multiplexing unit (IMU)_. The _FLMU_ is a fully connected layer taking _2n_ inputs and outputting _m_ signals. The _IMU_ takes _2m_ inputs and outputs _m_ signals.\n",
    "\n",
    "Again the order of the expert modules is randomized for each minibatch and for each hop.\n",
    "\n",
    "**TODO:** For the case where _k_ is odd we could have an alternate _FLMU_ taking _n+m_ inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "The communication channel has _n = 64_ signals. At each hop a module emits _m = 64_ signals. We use 1 hop. We use cascading multiplexing.\n",
    "\n",
    "The architecture of each module is the following:\n",
    "- `Input`: 28x28x1 MNIST images\n",
    "- `Conv1`: 35 filters of 5x5x1 (ReLu)\n",
    "- `Pool1`: Maxpool 2x2, output: 14x14x32\n",
    "- `Conv2`: 64 filters of 5x5x32 (ReLu)\n",
    "- `Pool2`: Maxpool 2x2, output: 7x7x64\n",
    "- `FC`: Fully connected layer of 64 ReLu\n",
    "- `COM`: Fully connected layer of 64 ReLu taking FC and emitting onto the communication channel\n",
    "- `Output`: Single 128-way Sigmoid (64 inputs from FC, 64 from the incoming communication channel)\n",
    "\n",
    "The `Conv*` and `Pool*` layers are shared between all expert module. The others are unique to each module.\n",
    "\n",
    "The cascading _MU_ is a 64 ReLu fully connected layer with 128 inputs.\n",
    "\n",
    "The Conv1, Conv2 layers are shared by every module. They're only trained during _protocol definition_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main graph code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv layers\n",
    "These calls define the shared convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"conv/l1\"):\n",
    "  conv_x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "  conv_l1_w = weight_var([5, 5, 1, 32], \"w\")\n",
    "  conv_l1_b = bias_var([32], \"b\")\n",
    "  conv_l1 = conv_layer(conv_x, conv_l1_w, conv_l1_b)\n",
    "  conv_p1 = pool_layer(conv_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"conv/l2\"):\n",
    "  conv_l2_w = weight_var([5, 5, 32, 64], \"w\")\n",
    "  conv_l2_b = bias_var([64], \"b\")\n",
    "  conv_l2 = conv_layer(conv_p1, conv_l2_w, conv_l2_b)\n",
    "  conv_p2 = pool_layer(conv_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_out = tf.reshape(conv_p2, [-1, 7 * 7 * 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module-specific layers\n",
    "\n",
    "These functions create the layers specific to a given expert module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Important, set tf.variable_scope before calling these functions.\n",
    "# In this scope, set |reuse = True| to load previously created\n",
    "# variables.\n",
    "\n",
    "# Creates the top part of the module: the FC and COM layers.\n",
    "# Returns the output of these layers in that order.\n",
    "def create_module_top():\n",
    "  global conv_out\n",
    "  with tf.variable_scope(\"fc\"):\n",
    "    fc_w = weight_var([7 * 7 * 64, 64], \"w\")\n",
    "    fc_b = bias_var([64], \"b\")\n",
    "    fc = fully_connected_layer(conv_out, fc_w, fc_b)\n",
    "  with tf.variable_scope(\"com\"):\n",
    "    com_w = weight_var([64, 64], \"w\")\n",
    "    com_b = bias_var([64], \"b\")\n",
    "    com = fully_connected_layer(fc, com_w, com_b)\n",
    "  return fc, com\n",
    "\n",
    "# Creates the bottom part of the module given the FC of this module and channel.\n",
    "# Return the output.\n",
    "def create_module_bottom(fc_module, channel):\n",
    "  with tf.variable_scope(\"out\"):\n",
    "    concat = tf.concat(1, [fc_module, channel])\n",
    "    output_w = weight_var([128, 1], \"w\")\n",
    "    output_b = weight_var([1], \"b\")\n",
    "    output = fully_connected_sigmoid_layer(concat, output_w, output_b)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication channel\n",
    "These function creates a single multiplexing unit. Every multiplexing unit share the same weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"mu\"):\n",
    "  mu_w = weight_var([128, 64], \"w\")\n",
    "  mu_b = bias_var([64], \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_multiplexing_initial_unit(input_from_module):\n",
    "    global mu_w, mu_b\n",
    "    # Keeping only the first 64 weights is equivalent to considering that the\n",
    "    # initial channel is filled with zeros.\n",
    "    output = fully_connected_layer(input_from_module, \n",
    "                                   tf.slice(mu_w, [0,0], [64,64]), mu_b)\n",
    "    return output\n",
    "\n",
    "def create_multiplexing_unit(input_from_module, input_from_channel):\n",
    "    global mu_w, mu_b\n",
    "    concat = tf.concat(1, [input_from_module, input_from_channel])\n",
    "    output = fully_connected_layer(concat, mu_w, mu_b)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connected modules\n",
    "\n",
    "Function for creating connected modules. Specify two sets of digits: the ones for which we should reuse existing modules, and the ones for which we should create existing modules. If you do not specify the full list of 10 digits then you must specify an other_class_name representing a module (which will be newly created) to classify the missing digits.\n",
    "\n",
    "The result is a tuple `(output, permutation)`. Where `output` is a tensor of shape `(?, n)` with `n` being the number of classes that can be distinguished. And where `permutation` is a placeholder for a tensor of shape `(n)` that should be populated with shuffled elements of `range(n)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_modules(digits_to_reuse, digits_to_create, other_class_name = None):\n",
    "  assert (len(digits_to_reuse) + len(digits_to_create) == 10) != \\\n",
    "         (other_class_name != None)\n",
    "  names_and_reuses = zip(\n",
    "    [\"digit%d\" % i for i in digits_to_reuse + digits_to_create],\n",
    "    [True] * len(digits_to_reuse) + [False] * len(digits_to_create))\n",
    "  if other_class_name != None:\n",
    "    names_and_reuses.append((other_class_name, False))\n",
    "\n",
    "  # Reuse or create the top of the modules.\n",
    "  modules_fc_and_com = []\n",
    "  for name,reuse in names_and_reuses:\n",
    "    with tf.variable_scope(name, reuse = reuse):\n",
    "      modules_fc_and_com.append(create_module_top())\n",
    "  \n",
    "  # Take the communication output of each channel and shuffle them to randomize\n",
    "  # the order in which they are added to the communication channel.\n",
    "  permutation = tf.placeholder(tf.int32, [len(modules_fc_and_com)])\n",
    "  packed_com = tf.pack([com for fc,com in modules_fc_and_com])\n",
    "  shuffled_com = tf.unpack(tf.gather(packed_com, permutation))\n",
    "  \n",
    "  # Multiplex the shuffled communication outputs one after the other onto the\n",
    "  # communication channel.\n",
    "  channel = create_multiplexing_initial_unit(shuffled_com[0])\n",
    "  for com in shuffled_com[1:]:\n",
    "    channel = create_multiplexing_unit(com, channel)\n",
    "   \n",
    "  # Reuse or create the bottom of the modules.\n",
    "  module_outputs = []\n",
    "  for (fc,com),(name,reuse) in zip(modules_fc_and_com, names_and_reuses):\n",
    "    with tf.variable_scope(name, reuse = reuse):\n",
    "      module_outputs.append(create_module_bottom(fc, channel))\n",
    "\n",
    "  # Softmax the output of each module.\n",
    "  output = tf.concat(1, module_outputs)\n",
    "  return tf.nn.softmax(output), permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered datasets\n",
    "\n",
    "Indicate which digits are recognized by expert modules trained during the **protocol definition** stage and which are left to expert modules trained during the **expert addition** stage. Create datasets appropriate for each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_digits = [0,1,  3,4,5,6,  8  ]\n",
    "ea_digits = [    2,        7,  9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_train_data = filter_data(train_data, pd_digits)\n",
    "pd_test_data = filter_data(test_data, pd_digits)\n",
    "ea_train_data = filter_data(train_data, pd_digits + ea_digits)\n",
    "ea_test_data = filter_data(test_data, pd_digits + ea_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protocol definition stage\n",
    "\n",
    "We create the connected modules for the protocol definition stage. We create modules for digits 0,1,3,4,6,7,8 and train them together with the multiplexing unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters for protocol definition\n",
    "pd_minibatch_size = 100\n",
    "pd_num_minibatches = 2000\n",
    "pd_learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_output, pd_permutation = create_modules([], pd_digits, 'other_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_y_ = tf.placeholder(tf.float32, [None, 8])\n",
    "\n",
    "pd_cross_entropy = tf.reduce_mean(\n",
    "    -tf.reduce_sum(pd_y_ * tf.log(pd_output), reduction_indices=[1]))\n",
    "\n",
    "pd_correct_prediction = tf.equal(tf.argmax(pd_output,1), tf.argmax(pd_y_,1))\n",
    "pd_accuracy = tf.reduce_mean(tf.cast(pd_correct_prediction, tf.float32))\n",
    "\n",
    "pd_train_step = tf.train.AdamOptimizer(pd_learning_rate).minimize(pd_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_trainer = Trainer(tf.trainable_variables(), x, pd_y_, pd_permutation, \n",
    "                     pd_minibatch_size, pd_train_step, pd_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_graph = pd_trainer.train(pd_train_data, pd_num_minibatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment and run manually to try just a couple more training steps.\n",
    "#pd_graph = pd_trainer.train(pd_train_data, 100, accuracy_every_n_steps = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%chart line --fields batch,accuracy --data pd_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd_trainer.test_accuracy(pd_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters for protocol definition\n",
    "ea_minibatch_size = 100\n",
    "ea_num_minibatches = 2000\n",
    "ea_learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ea_output, ea_permutation = create_modules(pd_digits, ea_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Freeze all variables, except trainable ones.\n",
    "ea_trainable_variables = []\n",
    "for digit in ea_digits:\n",
    "  ea_trainable_variables += tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                              'digit%d' % digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ea_y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "ea_cross_entropy = tf.reduce_mean(\n",
    "    -tf.reduce_sum(ea_y_ * tf.log(ea_output), reduction_indices=[1]))\n",
    "\n",
    "ea_correct_prediction = tf.equal(tf.argmax(ea_output,1), tf.argmax(ea_y_,1))\n",
    "ea_accuracy = tf.reduce_mean(tf.cast(ea_correct_prediction, tf.float32))\n",
    "\n",
    "ea_train_step = tf.train.AdamOptimizer(ea_learning_rate).minimize(\n",
    "    ea_cross_entropy, var_list = ea_trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ea_trainer = Trainer(ea_trainable_variables, x, ea_y_, ea_permutation, \n",
    "                     ea_minibatch_size, ea_train_step, ea_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ea_graph = ea_trainer.train(ea_train_data, ea_num_minibatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment and run manually to try just a couple more training steps.\n",
    "#ea_graph = ea_trainer.train(ea_train_data, 100, accuracy_every_n_steps = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%chart line --fields batch,accuracy --data ea_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ea_trainer.test_accuracy(ea_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
